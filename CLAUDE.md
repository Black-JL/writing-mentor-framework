# CLAUDE.md

This file provides guidance to Claude Code when grading student submissions in this folder.

## Purpose

This folder contains student submissions for grading. The primary task is evaluating these submissions against the assignment requirements and rubric, providing substantive feedback that teaches good thinking and good writing.

## Folder Structure

- `assignment.md` - Assignment instructions students received
- `rubric.md` - Grading rubric with point allocations
- `submissions/` - Student submission files (PDF, DOCX, XLSX, etc.)
- `turnitin/` - Turnitin similarity reports (optional; include when flagging for citation review)
- `skills/grading/` - Grading instructions, scripts, and references
- `grading_extracted/` - Extracted text from submissions (generated by prep script)
- `grading_rendered/` - Rendered Excel charts as images (generated by prep script)
- `grading_feedback/` - Individual student feedback files (one per student, isolated grading)
- `grading_report_*.md` - Consolidated grading reports (optional, combines individual feedback)

## Grading Workflow (Isolated Per Student)

To ensure fair, unbiased grading, each student is graded by a **separate agent with fresh context**. This prevents anchor effects, comparison bias, and context contamination between papers.

### Phase 1: Preparation (Run Once)

Before grading, run the extraction scripts to prepare all materials:

```bash
# Extract text from all submissions
python skills/grading/scripts/extract_submission_text.py --input submissions --out grading_extracted

# If submissions include Excel files, render charts for visual review
python skills/grading/scripts/render_xlsx_quicklook.py --input submissions --out grading_rendered
```

Then enumerate students by parsing filenames (Canvas format: `username_assignmentID_submissionID_filename`). Group files by username. For students with multiple submissions, identify the final version (highest submissionID).

### Phase 2: Isolated Grading (One Agent Per Student)

For **each student**, spawn a separate Task agent with `subagent_type: "general-purpose"`. Each agent receives:
- The reference materials (read fresh each time)
- **Only that student's files** (no other students' work)
- Instructions to write to a student-specific output file

**Agent prompt template:**

```
You are a distinguished expert mentoring a student on their written work. Grade with intellectual rigor while teaching clear thinking and effective writing.

VOICE: Write feedback as a demanding but generous mentor. Don't just check boxes; teach this student to think and write well. Connect errors to conceptual gaps, explain WHY issues matter, and show how improvements would strengthen their work.

INSTRUCTIONS: Read and follow the detailed grading workflow in skills/grading/SKILL.md - it contains examples of the feedback voice and the full output format.

REFERENCES (READ ALL BEFORE GRADING):
- Assignment requirements: assignment.md
- Grading rubric: rubric.md
- Economical writing principles: skills/grading/references/economical_writing_principles.md (REQUIRED - use to evaluate and teach good writing: clarity, active verbs, concrete examples, plain language)
- Course-specific concepts: skills/grading/references/course_concepts.md (if available - use to connect feedback to course material)

STUDENT: {username}

STUDENT FILES TO GRADE (read ONLY these - do not read other students' files):
- Extracted text: grading_extracted/{username}_*.txt
- Rendered charts: grading_rendered/{username}_*.png (if applicable)
- Original files in submissions/ matching {username}_*

SUBMISSION VERSIONS: {version_info}
- If multiple versions exist, grade only the FINAL version (highest submissionID)
- If prior grading_feedback/{username}.md exists, include revision assessment

TURNITIN: {turnitin_info}
- If a Turnitin report exists for this student, incorporate into feedback
- If not, proceed without (instructor chose not to include)

FEEDBACK MUST INCLUDE:
1. "What You Did Well" - genuine strengths showing good thinking AND good writing
2. "Developing Your Analysis" - 2-3 teaching-focused issues explaining WHY they matter
3. "Strengthening Your Writing" - if needed, 1-2 writing issues with teaching-focused explanations
4. "Required Revisions" - specific fixes prioritized by impact
5. "Optional Enhancements" - suggestions for excellent work
6. "Turnitin Review" (ONLY if Turnitin report provided)
7. "Revision Assessment" (ONLY if multiple submissions exist)

WRITING EVALUATION: Look for active verbs, concrete examples, plain language, and natural voice. Address nominalization, passive voice, elegant variation, and boilerplate. Frame feedback around WHY clear writing matters professionally.

OUTPUT: Write feedback to grading_feedback/{username}.md
```

### Phase 3: Consolidation (Optional)

After all agents complete, optionally combine individual feedback files:
```bash
cat grading_feedback/*.md > grading_report_$(date +%Y-%m-%d_%H%M%S).md
```

### Grading a Single Student

To grade just one student:
```
Grade the submission for {username}
```

This spawns one agent for that student only.

### Grading All Students

To grade all submissions with isolated context:
```
Grade all submissions with isolated context
```

**CRITICAL: Run agents SEQUENTIALLY, not in parallel.**

The workflow must process students one at a time:
1. Run Phase 1 (preparation) once
2. For each student:
   - Spawn ONE Task agent for that student
   - **WAIT for it to complete** before starting the next
   - Confirm the feedback file was written
3. After ALL agents complete, optionally run Phase 3 (consolidation)

**DO NOT spawn multiple agents in parallel.** Running parallel agents causes the system to appear stuck even when work completes successfully. Sequential execution is slower but reliable.

## Commands

Extract text from all submissions:
```bash
python skills/grading/scripts/extract_submission_text.py --input submissions --out grading_extracted
```

Render Excel charts (Quick Look method):
```bash
python skills/grading/scripts/render_xlsx_quicklook.py --input submissions --out grading_rendered
```

Render Excel charts (Excel automation method - preferred for chart fidelity):
```bash
python skills/grading/scripts/render_xlsx_excel.py --input submissions --out grading_rendered
```

## Dependencies

The extraction scripts require:
- `olefile` Python package (`pip install olefile`)
- `pdftotext` and `pdftoppm` from poppler (`brew install poppler`)
- `tesseract` for OCR fallback (`brew install tesseract`)

## Setup for a New Assignment

1. Copy this entire folder to your assignment location
2. Replace `assignment.md` with the actual assignment instructions
3. Replace `rubric.md` with the grading rubric for this assignment
4. (Optional) Add `skills/grading/references/course_concepts.md` with relevant course material to reference in feedback
5. Place student submissions in `submissions/`
6. Ask Claude to grade the submissions
