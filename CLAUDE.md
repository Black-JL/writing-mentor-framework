# CLAUDE.md

This file provides guidance to Claude Code when reviewing submissions in this folder.

## Purpose

This folder contains submissions for review using the Writing Mentor Framework. The primary task is:
1. **Validating data and calculations** across all submission components (papers, spreadsheets, charts, raw data)
2. **Checking internal and external consistency** against economic/statistical principles
3. **Providing two-tier feedback**: detailed technical notes for the reviewer, teaching-focused guidance for the writer

## Folder Structure

- `assignment.md` - Assignment/paper requirements
- `rubric.md` - Evaluation criteria with point allocations
- `submissions/` - Submission files (PDF, DOCX, XLSX, etc.)
- `turnitin/` - Similarity reports (optional)
- `skills/grading/` - Framework instructions, scripts, and references
- `feedback_extracted/` - Extracted text from submissions (generated by prep script)
- `feedback_rendered/` - Rendered Excel charts as images (generated by prep script)
- `feedback/` - Individual feedback files (one per writer, isolated review)

## Dependency Check (Run First Time)

Before running the framework for the first time, check that all dependencies are installed:

```bash
python skills/grading/scripts/check_dependencies.py
```

If any dependencies are missing, the script will list them and show the install commands. **Prompt the user** to install missing dependencies before proceeding:

```
Some dependencies are missing. Would you like me to install them?
- pip install olefile
- brew install poppler
- brew install tesseract
```

Only proceed with the review workflow after dependencies are confirmed.

## Review Workflow (Isolated Per Submission)

To ensure fair, unbiased review, each submission is reviewed by a **separate agent with fresh context**. This prevents anchor effects, comparison bias, and context contamination between papers.

### Phase 1: Preparation (Run Once)

Before reviewing, run the extraction scripts to prepare all materials:

```bash
# Extract text and formulas from all submissions
python skills/grading/scripts/extract_submission_text.py --input submissions --out feedback_extracted

# Render Excel charts for visual review and formula validation
python skills/grading/scripts/render_xlsx_quicklook.py --input submissions --out feedback_rendered
```

Then enumerate submissions by parsing filenames (Canvas format: `username_assignmentID_submissionID_filename`). Group files by username. For writers with multiple submissions, identify the final version (highest submissionID).

### Phase 2: Isolated Review (Parallel Agents)

For **each submission**, spawn a separate Task agent with `subagent_type: "general-purpose"`. Each agent receives:
- The reference materials (read fresh each time)
- **Only that writer's files** (no other submissions)
- Instructions to write to a submission-specific output file

**Parallelism:** Review submissions in batches based on `review.max_parallel_agents` in config (default: 3).
- Spawn up to N agents simultaneously
- Wait for the batch to complete before starting the next batch
- Each agent writes to a separate file, so there are no conflicts

**Agent prompt template:**

```
You are providing feedback using the Writing Mentor Framework. Your role goes beyond surface-level review—you validate underlying data and logic, check internal and external consistency, and deliver two-tier feedback.

VOICE: Write feedback as a demanding but generous mentor. Don't just check boxes; teach this writer to think and write well. Connect errors to conceptual gaps, explain WHY issues matter.

INSTRUCTIONS: Read and follow the detailed workflow in skills/grading/SKILL.md - it contains the two-tier output format, data validation procedures, and examples.

REFERENCES (READ ALL BEFORE REVIEWING):
- Assignment requirements: assignment.md
- Evaluation criteria: rubric.md
- Economical writing principles: skills/grading/references/economical_writing_principles.md (REQUIRED)
- Course/domain concepts: skills/grading/references/course_concepts.md (if available - enhances assumption validation)

WRITER: {username}

FILES TO REVIEW (read ONLY these - do not read other submissions):
- Extracted text/formulas: feedback_extracted/{username}_*.txt
- Rendered charts: feedback_rendered/{username}_*.png (if applicable)
- Original files in submissions/ matching {username}_*

SUBMISSION VERSIONS: {version_info}
- If multiple versions exist, review only the FINAL version (highest submissionID)
- If prior feedback/{username}.md exists, include revision assessment

TURNITIN: {turnitin_info}
- If a similarity report exists, incorporate into feedback
- If not, proceed without

DATA VALIDATION (CRITICAL):
1. Map the data pathway: Raw data → Calculations → Charts → Claims in paper
2. Check internal consistency: Do formulas match charts? Do paper claims match spreadsheet values?
3. Check external validity: Are assumptions correct? (nominal vs real GDP, PPP adjustment, etc.)
4. Attempt source verification: If data sources are cited, verify via API when possible (FRED, World Bank, etc.)
5. Document all validation findings in REVIEWER NOTES (Tier 1)

TWO-TIER OUTPUT:
- SECTION A (Reviewer Notes): Technical audit, exact issues, data validation findings, evidence trail. FOR INSTRUCTOR ONLY.
- SECTION B (Writer Feedback): Teaching-focused guidance with evidence but NOT exact fixes. Help writer discover issues themselves.

SYSTEMATIC EVALUATION: Before writing feedback, silently run through:
- Multi-Component Validation Checklist
- Economic/Statistical Assumption Audit
- Analysis Quality Checklist
- Evidence Quality Checklist
- Structure & Logic Checklist

REVIEWER NOTES MUST INCLUDE:
- Data Validation Summary (components reviewed, pathway map, consistency findings)
- Assumption Audit table (expected vs found)
- External Validation Attempts (API lookups, reference checks)
- Line-by-Line Issues with exact locations
- Evidence Trail (how you determined issues)

WRITER FEEDBACK MUST INCLUDE:
1. "Summary" - 2-3 sentence overall assessment
2. "Score" with Rubric Breakdown
3. "What You Did Well" - genuine strengths
4. "Developing Your Analysis" - issues with AREA OF CONCERN (not exact location) and QUESTION TO CONSIDER
5. "Strengthening Your Writing" - if needed
6. "Questions for Reflection" - prompts for deeper thinking
7. "Major/Minor Concerns" and "Suggestions" - severity-tiered
8. Additional sections as applicable (Code Review, Similarity Review, Revision Assessment)

OUTPUT: Write feedback to feedback/{username}.md
```

### Phase 3: Consolidation (Optional)

After all agents complete, optionally combine individual feedback files:
```bash
cat feedback/*.md > feedback_report_$(date +%Y-%m-%d_%H%M%S).md
```

### Reviewing a Single Submission

To review just one submission:
```
Review the submission for {username}
```

This spawns one agent for that submission only.

### Reviewing All Submissions

To review all submissions with isolated context:
```
Review all submissions with isolated context
```

**Process submissions in parallel batches.**

The workflow processes submissions in batches for speed while maintaining reliability:
1. Run Phase 1 (preparation) once
2. Read `max_parallel_agents` from config (default: 3)
3. For each batch of N submissions:
   - Spawn N Task agents **in a single message** (parallel)
   - **WAIT for all N to complete** before starting the next batch
   - Confirm all feedback files were written
4. After ALL agents complete, optionally run Phase 3 (consolidation)

**Parallelism settings** (in `wmf-config.yaml`):
- `max_parallel_agents: 3` — Default, good balance of speed and reliability
- `max_parallel_agents: 1` — Sequential mode if you experience issues
- `max_parallel_agents: 5-10` — For large classes with hundreds of students

## Commands

Extract text and formulas from all submissions:
```bash
python skills/grading/scripts/extract_submission_text.py --input submissions --out feedback_extracted
```

Render Excel charts (Quick Look method):
```bash
python skills/grading/scripts/render_xlsx_quicklook.py --input submissions --out feedback_rendered
```

Render Excel charts (Excel automation - preferred for chart fidelity):
```bash
python skills/grading/scripts/render_xlsx_excel.py --input submissions --out feedback_rendered
```

## Dependencies

The extraction scripts require:
- `olefile` Python package (`pip install olefile`)
- `pdftotext` and `pdftoppm` from poppler (`brew install poppler`)
- `tesseract` for OCR fallback (`brew install tesseract`)

## Setup for a New Project

1. Copy this entire folder to your project location
2. Replace `assignment.md` with the paper/assignment requirements
3. Replace `rubric.md` with your evaluation criteria
4. (Recommended) Add `skills/grading/references/course_concepts.md` with domain concepts to enhance assumption validation
5. Place submissions in `submissions/`
6. Ask Claude to review the submissions
