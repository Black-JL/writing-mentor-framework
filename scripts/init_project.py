#!/usr/bin/env python3
"""
Initialize a new project folder to use the Writing Mentor Framework.

This script sets up a project folder with:
- wmf-config.yaml pointing to the central framework
- Minimal CLAUDE.md with review instructions
- Template guidelines.md and criteria.md
- Empty submissions/ folder

Usage:
    python init_project.py --target /path/to/project/folder [--framework /path/to/framework]

If --framework is not specified, uses the parent directory of this script.
"""

import argparse
import sys
from pathlib import Path


def get_framework_path(script_path: Path) -> Path:
    """Get the framework root path from script location."""
    # Script is in framework/scripts/, so parent.parent is framework root
    return script_path.parent.parent


def create_config(target: Path, framework: Path) -> None:
    """Create wmf-config.yaml in the target folder."""
    config_content = f'''# Writing Mentor Framework - Configuration
# Generated by init_project.py

# Path to the framework installation
framework_path: {framework}

# Writing guidelines and evaluation criteria (in this folder)
guidelines: guidelines.md
criteria: criteria.md

# Optional: Domain-specific concepts for assumption validation
# Create this file or set to null to use general principles
domain_concepts: domain_concepts.md

# Submission handling
submissions:
  folder: submissions
  rounds:
    enabled: false

# Output folders (created automatically during review)
output:
  extracted: feedback_extracted
  rendered: feedback_rendered
  feedback: feedback

# Optional: Similarity reports
similarity_reports: similarity_reports

# Review settings
review:
  # Parallel agents: 3 = default, 1 = sequential/safe, 5-10 = large batches
  max_parallel_agents: 3
  compare_to_round: null
'''
    (target / 'wmf-config.yaml').write_text(config_content)
    print(f"  Created: wmf-config.yaml")


def create_claude_md(target: Path, framework: Path) -> None:
    """Create minimal CLAUDE.md in the target folder."""
    claude_content = f'''# CLAUDE.md

This folder uses the **Writing Mentor Framework** for reviewing writing.

## Configuration

Framework location: `{framework}`
Settings: See `wmf-config.yaml`

## Quick Start

1. Place submissions in `submissions/`
2. Ask Claude to review the submissions

## Commands

**Check dependencies:**
```bash
python {framework}/skills/feedback/scripts/check_dependencies.py
```

**Extract text from submissions:**
```bash
python {framework}/skills/feedback/scripts/extract_submission_text.py --input submissions --out feedback_extracted
```

**Render Excel charts:**
```bash
python {framework}/skills/feedback/scripts/render_xlsx_quicklook.py --input submissions --out feedback_rendered
```

## Review Workflow

When reviewing submissions, Claude Code should:

1. **Read `wmf-config.yaml`** to get framework path and settings
2. **Read framework instructions** from `{framework}/skills/feedback/SKILL.md`
3. **Read writing guidelines** from `guidelines.md`
4. **Read evaluation criteria** from `criteria.md`
5. **Read writing principles** from `{framework}/skills/feedback/references/economical_writing_principles.md`
6. **Read domain concepts** from `domain_concepts.md` (if present) for assumption validation
7. **Follow the isolated review workflow** in the framework SKILL.md

## Key Principles

- **Two-tier feedback**: Reviewer Notes (for reviewer) + Writer Feedback (for writer)
- **Data validation**: Check internal consistency, verify formulas match charts
- **Assumption auditing**: Check against economic/statistical principles
- **Isolated review**: Each submission reviewed by fresh agent (no cross-contamination)
- **Teaching focus**: Guide writers to discover fixes themselves

## For Multi-Round Reviews

1. Enable rounds in `wmf-config.yaml`
2. Use subfolders: `submissions/round1/`, `submissions/round2/`
3. Set `compare_to_round` in config for resubmission comparisons

See framework README for detailed instructions.
'''
    (target / 'CLAUDE.md').write_text(claude_content)
    print(f"  Created: CLAUDE.md")


def create_template_files(target: Path) -> None:
    """Create template guidelines.md and criteria.md."""

    guidelines_content = '''# Writing Guidelines

<!-- Replace this with your writing guidelines and requirements -->

## Overview

[Describe the purpose and context of this writing]

## Required Components

1. [Component 1]
2. [Component 2]
3. [Component 3]

## Formatting Requirements

- [Length, format, citation style, etc.]

## Deadline

[Date]

## Resources Provided

- [Any data files, readings, etc.]
'''

    criteria_content = '''# Evaluation Criteria

<!-- Replace this with your evaluation criteria -->

## Total Points: [X]

### [Criterion 1] ([X] points)

| Score | Description |
|-------|-------------|
| Excellent (X) | [What excellent work looks like] |
| Competent (X) | [What competent work looks like] |
| Developing (X) | [What developing work looks like] |
| Insufficient (X) | [What insufficient work looks like] |

### [Criterion 2] ([X] points)

| Score | Description |
|-------|-------------|
| Excellent (X) | [Description] |
| Competent (X) | [Description] |
| Developing (X) | [Description] |
| Insufficient (X) | [Description] |

<!-- Add more criteria as needed -->
'''

    domain_concepts_content = '''# Domain Concepts

<!-- Optional: Add domain-specific concepts for enhanced assumption validation -->
<!-- Without this file, the framework still provides full feedback on writing quality, -->
<!-- analysis structure, and data validation - it just won't catch domain-specific errors -->

## Key Principles

- [Principle 1]
- [Principle 2]

## Common Errors to Flag

- [Error type 1: description and correct approach]
- [Error type 2: description and correct approach]

## Terminology

- **Term 1**: Definition
- **Term 2**: Definition

## Example: Economics

```
## GDP Comparisons
- Cross-time comparisons require real (inflation-adjusted) GDP, not nominal
- Cross-country comparisons require PPP adjustment
- Per-capita normalization needed for different population sizes

## Statistical Claims
- Correlation does not imply causation
- Statistical significance requires appropriate tests
```
'''

    (target / 'guidelines.md').write_text(guidelines_content)
    print(f"  Created: guidelines.md (template)")

    (target / 'criteria.md').write_text(criteria_content)
    print(f"  Created: criteria.md (template)")

    (target / 'domain_concepts.md').write_text(domain_concepts_content)
    print(f"  Created: domain_concepts.md (template)")


def create_folders(target: Path) -> None:
    """Create necessary folders."""
    folders = ['submissions', 'similarity_reports']
    for folder in folders:
        (target / folder).mkdir(exist_ok=True)
        # Add .gitkeep to empty folders
        gitkeep = target / folder / '.gitkeep'
        if not gitkeep.exists():
            gitkeep.touch()
    print(f"  Created: submissions/, similarity_reports/")


def install_feedback_skill(target: Path, framework: Path) -> None:
    """Install the /feedback skill into the target folder."""
    skill_dir = target / 'skills' / 'feedback'
    skill_dir.mkdir(parents=True, exist_ok=True)

    skill_content = f'''---
name: feedback
description: Review writing submissions using the Writing Mentor Framework
user-invocable: true
---

# Review Submissions

Review all submissions in this folder using the Writing Mentor Framework.

## Prerequisites

Before running, ensure you have:
- `guidelines.md` - Your writing guidelines and requirements
- `criteria.md` - Your evaluation criteria
- `submissions/` - Folder containing writing to review

Optional:
- `domain_concepts.md` - Domain concepts for assumption validation
- `similarity_reports/` - Similarity reports

## Workflow

When this skill is invoked:

1. **Read the config** from `wmf-config.yaml` to get the framework path: `{framework}`

2. **Check dependencies** by running:
   ```bash
   python {framework}/skills/feedback/scripts/check_dependencies.py
   ```
   If dependencies are missing, prompt the user to install them before continuing.

3. **Verify required files exist**:
   - `guidelines.md` (required)
   - `criteria.md` (required)
   - `submissions/` folder with at least one file (required)

   If any are missing, tell the user what's needed and stop.

4. **Extract text from submissions**:
   ```bash
   python {framework}/skills/feedback/scripts/extract_submission_text.py --input submissions --out feedback_extracted
   ```

5. **Render Excel charts** (if any .xlsx files exist):
   ```bash
   python {framework}/skills/feedback/scripts/render_xlsx_quicklook.py --input submissions --out feedback_rendered
   ```

6. **Read the framework instructions** from `{framework}/skills/feedback/SKILL.md`

7. **Read reference materials**:
   - `guidelines.md`
   - `criteria.md`
   - `{framework}/skills/feedback/references/economical_writing_principles.md`
   - `domain_concepts.md` (if present)

8. **Enumerate submissions** by parsing filenames. Group by writer (filename prefix before the first underscore).

9. **Read parallelism setting** from `wmf-config.yaml`: `review.max_parallel_agents` (default: 3)

10. **Process submissions in parallel batches**:
    - For each batch of N submissions (where N = max_parallel_agents):
      - Spawn N Task agents **in a single message** with `subagent_type: "general-purpose"`
      - Each agent follows the isolated review workflow from the framework SKILL.md
      - **Wait for all N to complete** before starting the next batch
    - Each agent writes to `feedback/{{username}}.md`

11. **Report completion** with summary of submissions reviewed.

**Parallelism settings** (in `wmf-config.yaml`):
- `max_parallel_agents: 3` — Default, good balance of speed and reliability
- `max_parallel_agents: 1` — Sequential mode if you experience rate limits or errors
- `max_parallel_agents: 5-10` — For large batches with hundreds of submissions

## Multi-Round Reviews

If using rounds (`submissions/round1/`, `submissions/round2/`):

1. Check `wmf-config.yaml` for `submissions.rounds.enabled`
2. Use round-specific folders: `feedback_extracted_round{{N}}/`, `feedback_round{{N}}/`
3. For resubmissions, compare to prior round feedback per `review.compare_to_round` config setting

## Output

Feedback files are written to `feedback/` (or `feedback_round{{N}}/` for rounds).

Each file contains:
- **Section A: Reviewer Notes** - Technical audit for reviewer only
- **Section B: Writer Feedback** - Teaching-focused guidance to share with writer
'''
    (skill_dir / 'SKILL.md').write_text(skill_content)
    print(f"  Created: skills/feedback/SKILL.md (invoke with /feedback)")


def main():
    parser = argparse.ArgumentParser(
        description='Initialize a new project folder for the Writing Mentor Framework'
    )
    parser.add_argument(
        '--target', '-t',
        required=True,
        help='Target folder to initialize (will be created if it does not exist)'
    )
    parser.add_argument(
        '--framework', '-f',
        help='Path to the framework installation (default: parent of this script)'
    )
    parser.add_argument(
        '--force',
        action='store_true',
        help='Overwrite existing files'
    )
    args = parser.parse_args()

    target = Path(args.target).expanduser().resolve()
    script_path = Path(__file__).resolve()

    if args.framework:
        framework = Path(args.framework).expanduser().resolve()
    else:
        framework = get_framework_path(script_path)

    # Validate framework path
    skill_file = framework / 'skills' / 'feedback' / 'SKILL.md'
    if not skill_file.exists():
        print(f"Error: Framework not found at {framework}")
        print(f"  Expected to find: {skill_file}")
        sys.exit(1)

    # Create target folder if needed
    target.mkdir(parents=True, exist_ok=True)

    # Check for existing files
    existing_files = []
    for f in ['wmf-config.yaml', 'CLAUDE.md', 'guidelines.md', 'criteria.md']:
        if (target / f).exists():
            existing_files.append(f)

    if existing_files and not args.force:
        print(f"Error: Target folder already contains: {', '.join(existing_files)}")
        print("  Use --force to overwrite existing files")
        sys.exit(1)

    print(f"\nInitializing Writing Mentor Framework project:")
    print(f"  Target: {target}")
    print(f"  Framework: {framework}")
    print()

    create_config(target, framework)
    create_claude_md(target, framework)
    create_template_files(target)
    create_folders(target)
    install_feedback_skill(target, framework)

    print()
    print("=" * 50)
    print("Project initialized successfully!")
    print("=" * 50)
    print()
    print("NEXT STEPS:")
    print()
    print("  1. DROP IN your files:")
    print("     - guidelines.md  <- Your writing guidelines and requirements")
    print("     - criteria.md    <- Your evaluation criteria")
    print("     - submissions/   <- Writing to review (DOCX, PDF, XLSX)")
    print()
    print("  2. (Optional) Edit domain_concepts.md with domain principles")
    print()
    print("  3. OPEN Claude Code:")
    print(f"     cd {target}")
    print("     claude")
    print()
    print("  4. TYPE: /feedback")
    print("     (or say: 'review the submissions')")


if __name__ == '__main__':
    main()
