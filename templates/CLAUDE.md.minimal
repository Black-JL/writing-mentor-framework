# CLAUDE.md

This folder uses the **Writing Mentor Framework** for reviewing submissions.

## Configuration

Framework location and settings are defined in `wmf-config.yaml`.

## Quick Commands

**Check dependencies:**
```bash
python {FRAMEWORK_PATH}/skills/grading/scripts/check_dependencies.py
```

**Extract text from submissions:**
```bash
python {FRAMEWORK_PATH}/skills/grading/scripts/extract_submission_text.py --input submissions --out grading_extracted
```

**Render Excel charts:**
```bash
python {FRAMEWORK_PATH}/skills/grading/scripts/render_xlsx_quicklook.py --input submissions --out grading_rendered
```

## Review Workflow

When asked to review submissions, Claude Code should:

1. **Read the config file** (`wmf-config.yaml`) to get framework path and settings
2. **Read framework instructions** from `{framework_path}/skills/grading/SKILL.md`
3. **Read assignment requirements** from `assignment.md` (in this folder)
4. **Read evaluation criteria** from `rubric.md` (in this folder)
5. **Read writing principles** from `{framework_path}/skills/grading/references/economical_writing_principles.md`
6. **Read course concepts** from `course_concepts.md` (if present) or `{framework_path}/skills/grading/references/course_concepts.md`
7. **Follow the isolated review workflow** described in the framework SKILL.md

## Key Principles

- **Two-tier feedback**: Reviewer Notes (for instructor) + Writer Feedback (for student)
- **Data validation**: Check internal consistency and external validity
- **Isolated review**: Each submission reviewed by a fresh agent to prevent bias
- **Teaching focus**: Guide writers to discover fixes themselves

## For Multi-Round Reviews

If using rounds (submissions/round1/, round2/, etc.):

1. Update `wmf-config.yaml` with `rounds.enabled: true`
2. Set `review.compare_to_round` to the prior round number
3. Use round-specific output folders: `grading_extracted_round{N}`, etc.

See the framework README for detailed multi-round workflow instructions.
